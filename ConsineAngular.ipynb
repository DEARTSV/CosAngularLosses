{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CosFace, ArcFace\n",
    "\n",
    "Косинусно-угловые функции потерь: \n",
    "* CosFace (http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_CosFace_Large_Margin_CVPR_2018_paper.pdf, CVPR)\n",
    "* ArcFace (https://arxiv.org/abs/1801.07698).\n",
    "\n",
    "Наиболее актуальным нейросетевым методом для распознавания лиц (да и идентификации любых других сущностей) являются косинусно-угловые методы. Как и другие популярные подходы к обучению метрик (triplet и contrastive loss), они обладают хорошей обобщающей способностью и обучают непосредственно вектор-представление, однако не требуют сложных метоов составления пар или триплетов, являясь практически такими же простыми в использовании и обучении, как обычная классификация с использованием перекрёстной энтропии и softmax (softmax loss).\n",
    "\n",
    "Softmax loss можно представить следующим образом:\n",
    "\n",
    "$$ - \\frac{1}{N} \\sum_{i=1}^{N} \\log{\\frac{e^{W_{y_i}^{T} x_i + b_{y_i}} }{\\sum_{j=1}^{N} e^{W_{y_i}^{T} x_j + b_j}  }}$$\n",
    "\n",
    "\n",
    "Здесь N - размер батча, n - количество классов, $x_i \\in R^d$ вектор-эмбеддинг, соответствующий классу $y_i$, $W \\in R^{d\\times n}$ - веса между последним и предпоследним полносвязными слоями, $W_j \\in R^d$ - j-ый столбец матрицы $W$, который представляет основанный на эмбеддинге линейный классификатор для соответствующего класса, $b_i \\in R^n$ - смещение.\n",
    "\n",
    "Подходы данной группы предполагают, что $W_j$ представляют центры классов в угловом пространстве, и представляют $W_j x$ как $||W|| * ||x|| * cos(\\theta_j)$, где $\\theta_j$ - угол между $W_j$ и $x$. И CosFace, и ArcFace нормализуют как $x$, так и $W_j$, а также фиксируют смещение, делая его равным нулю, что практически не отражается на результате, после чего softmax loss начинает зависеть только от $cos(\\theta_j)$ :\n",
    "\n",
    "$$ - \\frac{1}{N} \\sum_{i=1}^{N} \\log{ \\frac{e^{\\cos(\\theta_{y_i})} }{e^{\\cos(\\theta_{y_i})} + \\sum_{j=1,j\\neq y_i}^{N} e^{\\cos(\\theta_j)} } } $$\n",
    "\n",
    "\n",
    "Они также вводят понятие scale, отвечающее за масштабирование получающихся эмбеддингов: $||x|| = scale$, а также угловой зазор margin, отвечающий за дискриминативность получающихся эмбеддингов (помогает “расталкивать” соответствующие классам группы получившихся эмбеддингов друг от друга в угловом пространстве), чего не хватает подходу с обычной классификацией. Для CosFace итоговая функция потерь выглядит следующим образом:\n",
    "\n",
    "$$ - \\frac{1}{N} \\sum_{i=1}^{N} \\log{\\frac{e^{scale \\times (\\cos(\\theta_{y_i})-margin)} }{e^{scale \\times (\\cos(\\theta_{y_i})-margin)} + \\sum_{j=1,j \\neq y_i}^{N} e^{scale \\times \\cos(\\theta_j)} } }$$\n",
    "\n",
    "\n",
    "ArcFace же непосредственно добавляет margin к получившемуся углу:\n",
    "\n",
    "$$ - \\frac{1}{N} \\sum_{i=1}^{N} \\log{\\frac{e^{ scale \\times \\cos(\\theta_{y_i}+margin)} }{e^{scale \\times \\cos(\\theta_{y_i}+margin)} + \\sum_{j=1,j \\neq y_i}^{N} e^{scale \\times \\cos(\\theta_j)} } }$$\n",
    "\n",
    "При этом вычислить угол в случае использования нормализации крайне просто:\n",
    "\n",
    "$$ \\cos(\\theta_{j}) = \\frac{W_j \\circ x}{||W_j|| \\times ||x||} = W_j \\circ x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Код\n",
    "CosFace и ArcFace реализованы с использованием keras (tensorflow backend), в качестве игрушечного примера взян MNIST. Имплементировано как слой, так как иначе Keras API не позволяет передать веса функции потерь. Предсказание класса по получаемому сетью эмбеддингу осуществляется с помощью KNN и эмбеддингов, полученных из тренировочных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.layers import Layer, Input, Activation, BatchNormalization, Conv2D, Dense, Flatten, Lambda\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.models import Model\n",
    "import math\n",
    "\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adagrad\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseCosFaceLoss(Layer):\n",
    "    def __init__(self, output_dim, margin=30, scale=0.35, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        super(DenseCosFaceLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        super(DenseCosFaceLoss, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        self.kernel = tf.nn.l2_normalize(self.kernel, 0, 1e-10)\n",
    "        x = tf.nn.l2_normalize(x, 1, 1e-10)\n",
    "        self.cos_t = tf.matmul(x, self.kernel)\n",
    "        return self.cos_t\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def loss(self, labels, _features):\n",
    "        labels = tf.cast(tf.squeeze(labels), tf.uint8)\n",
    "        labels = tf.one_hot(labels, self.output_dim, on_value=1.0, off_value=0.0, axis=-1, dtype=tf.float32)\n",
    "        \n",
    "        cosine = tf.clip_by_value(self.cos_t, -1, 1) - self.margin * labels\n",
    "        \n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=self.scale * cosine))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim,\n",
    "                  'm': self.margin,\n",
    "                  'scale': self.scale}\n",
    "        base_config = super(DenseCosFaceLoss, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DenseArcFaceLoss(Layer):\n",
    "    def __init__(self, output_dim, margin=64, scale=0.5, **kwargs):\n",
    "        # margin should be in [0, pi/2)\n",
    "        self.output_dim = output_dim\n",
    "        self.margin = margin\n",
    "        self.scale = scale\n",
    "        super(DenseArcFaceLoss, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(input_shape[1], self.output_dim),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        super(DenseArcFaceLoss, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        self.kernel = tf.nn.l2_normalize(self.kernel, 0, 1e-10)\n",
    "        x = tf.nn.l2_normalize(x, 1, 1e-10)\n",
    "        self.cos_t = tf.matmul(x, self.kernel)\n",
    "        return self.cos_t\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def loss(self, labels, _features):\n",
    "        labels = tf.cast(tf.squeeze(labels), tf.uint8)\n",
    "        labels = tf.one_hot(labels, self.output_dim, on_value=1.0, off_value=0.0, axis=-1, dtype=tf.float32)\n",
    "        \n",
    "        cos_m = math.cos(self.margin)\n",
    "        sin_m = math.sin(self.margin)\n",
    "\n",
    "        # get cos(t + m) without uning arccos(cos(t)). arccos is bad computationally\n",
    "        sin_t = tf.sqrt(1. - tf.square(self.cos_t))\n",
    "        cos_margined = self.scale * (cos_m * self.cos_t - sin_m * sin_t)\n",
    "\n",
    "        # for \"cos(t1 + m) > cos(t2)\" we want \"t1 + m\" to be in [0, pi]: otherwise m only makes \"cos(t1 + m)\" bigger,\n",
    "        # and it's not what is margin for. So when cos(t) < cos(pi - m) -> cos(t + m) > pi, and in this case\n",
    "        # we will just use cosface with some adaptive cosface margin built from arcface margin via m * sin(m)\n",
    "        threshold = math.cos(math.pi - self.margin)\n",
    "        switch_cosface = tf.to_float(self.cos_t >= threshold) * (self.cos_t - sin_m * self.margin)\n",
    "\n",
    "        arc = self.cos_t * (1. - labels) + tf.where(switch_cosface > 0., cos_margined, switch_cosface) * labels\n",
    "        return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=self.scale * arc))\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'output_dim': self.output_dim,\n",
    "                  'm': self.margin,\n",
    "                  'scale': self.scale}\n",
    "        base_config = super(DenseArcFaceLoss, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    \n",
    "def get_mnist_model(input_shape=(28, 28, 1), n_classes=10, mode='cosface', train=False):\n",
    "    img = Input(input_shape)\n",
    "\n",
    "    x = Conv2D(32, (3, 3), strides=(2, 2), padding='same', name='conv_1', kernel_initializer=glorot_uniform())(img)\n",
    "    x = BatchNormalization(axis=3, name='bn_1')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='same', name='conv_2', kernel_initializer=glorot_uniform())(x)\n",
    "    x = BatchNormalization(axis=3, name='bn_2')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv2D(64, (3, 3), strides=(2, 2), padding='valid', name='conv_3', kernel_initializer=glorot_uniform())(x)\n",
    "    x = BatchNormalization(axis=3, name='bn_3')(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu', kernel_initializer=glorot_uniform())(x)\n",
    "    \n",
    "    if train:\n",
    "        if mode == 'cosface':\n",
    "            x = DenseCosFaceLoss(n_classes, margin=30, scale=0.35)(x)\n",
    "        elif mode == 'arcface':\n",
    "            x = DenseArcFaceLoss(n_classes, margin=64, scale=0.5)(x)\n",
    "    else:\n",
    "        x = Lambda(lambda x:  tf.nn.l2_normalize(x, 1, 1e-10))(x)\n",
    "        \n",
    "\n",
    "    model = Model(inputs=img, outputs=x, name='mnist_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\_Sergey\\Soft\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv_1 (Conv2D)              (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "bn_2 (BatchNormalization)    (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv_3 (Conv2D)              (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "bn_3 (BatchNormalization)    (None, 3, 3, 64)          256       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_arc_face_loss_1 (Dense (None, 10)                640       \n",
      "=================================================================\n",
      "Total params: 93,952\n",
      "Trainable params: 93,632\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From <ipython-input-2-f43f05b0bb98>:84: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\_Sergey\\Soft\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 17s 278us/step - loss: 31.4659\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.3431\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.2953\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.2624\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.2373\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 31.2174\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.2013\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1883\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1776\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1689\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 31.1620\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1562\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 31.1515\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1472\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1437\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 31.1405\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1377\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1352\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1328\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 68us/step - loss: 31.1306\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1285\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1266\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1249\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 69us/step - loss: 31.1234\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 70us/step - loss: 31.1221\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 71us/step - loss: 31.1209\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 72us/step - loss: 31.1197\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 73us/step - loss: 31.1188\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 31.1182\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 74us/step - loss: 31.1173\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 5s 75us/step - loss: 31.1169\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 31.1162\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 31.1158\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 31.1155\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 5s 78us/step - loss: 31.1151\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 31.1151\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 31.1147\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 31.1146\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 31.1144\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 31.1142\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 31.1141\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 5s 82us/step - loss: 31.1141\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 31.1140\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 31.1141\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 31.1137\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 5s 84us/step - loss: 31.1137\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 31.1136\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 6s 92us/step - loss: 31.1135\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 31.1139\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 31.1136\n"
     ]
    }
   ],
   "source": [
    "def train_cosangular(learning_rate=0.01, batch_size=50, epochs=20, mode='cosface'):\n",
    "    if not os.path.isdir('data/training'):\n",
    "        os.makedirs('data/training')\n",
    "        \n",
    "    model = get_mnist_model(mode=mode, train=True)\n",
    "    model.summary()\n",
    "    model.compile(optimizer=Adam(learning_rate), loss=model.layers[-1].loss)\n",
    "    \n",
    "    model.fit(np.expand_dims(x_train, axis=-1), y_train, batch_size=batch_size, epochs=epochs,\n",
    "              callbacks=[ModelCheckpoint(filepath=os.path.join('data', 'training', 'checkpoint-{epoch:02d}.h5'),\n",
    "                                         save_weights_only=True)])\n",
    "    \n",
    "train_cosangular(learning_rate=0.0001, batch_size=300, epochs=50, mode='arcface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.9621\n"
     ]
    }
   ],
   "source": [
    "def validate_cosangular():\n",
    "    model = get_mnist_model(train=False)\n",
    "    model.load_weights('data/trained_weights.h5', by_name=True, skip_mismatch=True)\n",
    "    \n",
    "    train_embeddings = model.predict(np.expand_dims(x_train, axis=-1))\n",
    "    val_embeddings = model.predict(np.expand_dims(x_test, axis=-1))\n",
    "    \n",
    "    KNN = KNeighborsClassifier(n_neighbors=50, metric='sqeuclidean', weights='distance')\n",
    "    KNN.fit(train_embeddings, y_train)\n",
    "    \n",
    "    pred = KNN.predict(val_embeddings)\n",
    "    \n",
    "    acc = sum(y_test == pred) / len(pred)\n",
    "    print('accuracy: ', acc)\n",
    "    \n",
    "validate_cosangular()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
